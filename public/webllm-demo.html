<!DOCTYPE html>
<html lang="nl">
  <head>
    <meta charset="UTF-8" />
    <title>WebLLM chatdemo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="/webllm-demo.css" />
  </head>
  <body>
    <h3>Lokale LLM via WebGPU</h3>
    <div id="status">Status: model laden (eerste keer kan 20–40s duren)…</div>

    <div id="chat" class="chat"></div>

    <div class="row">
      <input
        id="prompt"
        placeholder="Voorbeeld: Explain what Edge AI is in one sentence."
      />
      <button id="go" disabled>Verstuur</button>
    </div>

    <p class="hint">
      Deze demo gebruikt een compact chatmodel (<code>Qwen1.5-0.5B-Chat</code>)
      via Transformers.js.
      (Edge&nbsp;AI).
    </p>

    <script type="module">
      import {
        pipeline,
        env,
      } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers/dist/transformers.min.js'

      // Cache mag NU wél aan – handig voor heropenen zonder opnieuw te downloaden
      env.useBrowserCache = true
      env.allowLocalModels = false
      env.allowRemoteModels = true

      const statusEl = document.getElementById('status')
      const chatEl = document.getElementById('chat')
      const promptEl = document.getElementById('prompt')
      const goBtn = document.getElementById('go')

      function addMessage(text, who) {
        const row = document.createElement('div')
        row.className = 'msg-row'

        const bubble = document.createElement('div')
        bubble.className = 'msg ' + who
        bubble.textContent = text

        row.appendChild(bubble)
        chatEl.appendChild(row)
        chatEl.scrollTop = chatEl.scrollHeight
      }

      // Typing indicator (spinner met 3 puntjes)
      let typingRow = null
      function showTyping() {
        if (typingRow) return
        typingRow = document.createElement('div')
        typingRow.className = 'msg-row'

        const bubble = document.createElement('div')
        bubble.className = 'msg bot typing'
        bubble.innerHTML =
          '<span class="dot"></span><span class="dot"></span><span class="dot"></span>'

        typingRow.appendChild(bubble)
        chatEl.appendChild(typingRow)
        chatEl.scrollTop = chatEl.scrollHeight
      }

      function hideTyping() {
        if (!typingRow) return
        chatEl.removeChild(typingRow)
        typingRow = null
      }

      let generator = null

      // Model laden (Qwen chatmodel)
      ;(async () => {
        try {
          statusEl.textContent =
            'Status: model laden (eerste keer kan 20–40s duren)…'

          generator = await pipeline(
            'text-generation',
            'Xenova/Qwen1.5-0.5B-Chat',
            { device: 'webgpu' } // valt terug op WASM/CPU als WebGPU niet kan
          )

          statusEl.textContent =
            'Status: klaar – model draait lokaal (WebGPU indien beschikbaar).'
          goBtn.disabled = false
        } catch (err) {
          console.error(err)
          statusEl.textContent =
            'Kon het model niet laden (mogelijk geblokkeerde download of netwerkprobleem).'
          addMessage('Fout bij laden model: ' + err.message, 'bot')
        }
      })()

      async function runGeneration() {
        const userText = promptEl.value.trim()
        if (!userText) return

        // User-bubbel komt meteen in beeld
        addMessage(userText, 'user')
        promptEl.value = ''
        promptEl.focus()

        if (!generator) {
          addMessage(
            'Model is nog aan het laden, probeer het over enkele seconden opnieuw.',
            'bot'
          )
          return
        }

        goBtn.disabled = true
        statusEl.textContent = 'Status: bezig met genereren…'
        showTyping()

        // Chat template voor Qwen
        const messages = [
          { role: 'system', content: 'You are a helpful assistant.' },
          { role: 'user', content: userText },
        ]

        const chatText = generator.tokenizer.apply_chat_template(messages, {
          tokenize: false,
          add_generation_prompt: true,
        })

        try {
          const output = await generator(chatText, {
            max_new_tokens: 160,
            do_sample: true,
            temperature: 0.7,
            return_full_text: false,
          })

          const answer = (output[0]?.generated_text || '').trim()
          hideTyping()
          addMessage(answer || '[Geen zinvol antwoord]', 'bot')
        } catch (err) {
          console.error(err)
          hideTyping()
          addMessage('Fout tijdens generatie: ' + err.message, 'bot')
        } finally {
          goBtn.disabled = false
          statusEl.textContent =
            'Status: klaar – model draait lokaal (WebGPU indien beschikbaar).'
        }
      }

      goBtn.addEventListener('click', runGeneration)

      // Enter om te verzenden (Shift+Enter voor nieuwe regel)
      promptEl.addEventListener('keydown', (e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
          e.preventDefault()
          runGeneration()
        }
      })
    </script>
  </body>
</html>
